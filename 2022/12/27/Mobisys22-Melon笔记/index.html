

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/logo.png">
  <link rel="icon" href="/img/logo.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Peter Wan">
  <meta name="keywords" content="">
  
    <meta name="description" content="2022-12-26 第6次组会汇报内容">
<meta property="og:type" content="article">
<meta property="og:title" content="Mobisys22 Melon笔记">
<meta property="og:url" content="http://seupeter.cn/2022/12/27/Mobisys22-Melon%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Peter&#39;s Blog">
<meta property="og:description" content="2022-12-26 第6次组会汇报内容">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221211818878.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221212605094.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221212642249.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221213714568.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221215815035.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221224148015.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221230605606.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221232121580.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221233043499.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222161555781.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222164639140.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222165635792.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222170146596.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222170958494.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222171615052.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222172934586.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222173253220.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222173543993.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222173619623.png">
<meta property="og:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222174919246.png">
<meta property="article:published_time" content="2022-12-26T17:01:14.000Z">
<meta property="article:modified_time" content="2022-12-30T16:42:14.242Z">
<meta property="article:author" content="Peter Wan">
<meta property="article:tag" content="MobiSys22">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221211818878.png">
  
  
  
  <title>Mobisys22 Melon笔记 - Peter&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"seupeter.cn","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Peter的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/back.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Mobisys22 Melon笔记"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Peter Wan
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-12-27 01:01" pubdate>
          2022年12月27日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          18k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          150 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Mobisys22 Melon笔记</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="Mobisys22-Melon笔记"><a href="#Mobisys22-Melon笔记" class="headerlink" title="Mobisys22 Melon笔记"></a>Mobisys22 Melon笔记</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>设备上学习是一种很有前途的新兴隐私保护机器学习范式技术。然而，通过定量实验，&#x3D;&#x3D;我们发现商品移动设备由于本地内存容量有限，不能很好地支持足够大的批处理大小的最先进的DNN训练。&#x3D;&#x3D;为了填补这一空白，我们提出了Melon，这是一种内存友好的设备上学习框架，它使大批量训练任务超出物理内存容量。Melon明智地改进了现有的内存保存技术，以适应资源受限的移动设备，即重新计算和微批处理。Meon 进一步结合了新技术来处理高内存碎片和内存适应。Melon进一步结合了新的技术来处理高内存碎片和内存适应。我们在商用移动设备上使用各种典型的DNN模型实现和评估Melon。结果表明，在相同的内存预算下，Melon 可以实现高达 4.33× 的批量大小。给定相同的批量大小，Melon 平均实现了 1.89 倍（高达 4.01×）更高的训练吞吐量，并且与竞争替代方案相比节省了高达 49.43% 的能量。此外，Melon 在内存预算适应方面平均减少了 78.59% 的计算量。</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 <strong>Introduction</strong></h3><p>深度神经网络 (DNN) 是当今移动应用程序的关键组成部分，例如语音助手、增强现实等。大量工作探索了如何利用强大的硬件和各种优化将 DNN 的推理阶段带入移动设备。作为向前迈出的一步，设备上的学习正在成为直接在移动设备上执行模型训练的新范式，特别是实现强大的&#x3D;&#x3D;隐私保护和个性化&#x3D;&#x3D;。它已经成为高级学习技术(如联邦学习、分裂学习等[41,60,67])和应用(如输入法、虚拟助手等[1,2,44]的基础。然而，由于本地硬件资源的限制，我们很直观地会问，现代DNN的训练在移动设备上是否负担得起。</p>
<p>不幸的是，正如我们将在§2中定量显示的那样，即使是具有8GB内存的高端移动设备也无法支持足够大的批量DNN训练，这对于实现高精度和稳定收敛是至关重要的。换句话说，&#x3D;&#x3D;内存墙阻碍了训练性能&#x3D;&#x3D;。在联邦学习中，这种内存不足将被放大，因为低端设备将是端到端收敛的瓶颈。为此，我们的目标是通过内存优化技术打破内存墙。</p>
<h5 id="Prior-wisdom"><a href="#Prior-wisdom" class="headerlink" title="Prior wisdom"></a>Prior wisdom</h5><p>我们注意到，模型训练的内存优化多年来在云计算中得到了广泛的研究，但在移动设备中很少讨论。因此，我们的第一个直觉是调查最成熟的云端内存优化技术是否可以用于移动设备。令我们惊讶的是，我们在§3 中的定量实验表明，云端技术很难应用于移动设备</p>
<ol>
<li>交换[33,42,52,74]引入了严重的同步开销，因为移动soc缺乏像服务器gpu这样的高速I&#x2F;O链路(如PCIe)</li>
<li>训练时的压缩极大地损害了模型的准确性，尤其是在联合设置中</li>
</ol>
<h5 id="Our-design"><a href="#Our-design" class="headerlink" title="Our design"></a>Our design</h5><p>我们提出了Melon，这是第一个可以实际部署在移动设备上的内存优化DNN训练框架。Melon在内存预算(应用程序或操作系统指定的训练过程可用内存的大小)下限制了峰值内存使用。Melo在理想情况下，当内存预算是无限的，不会产生任何精度下降，并获得相当的性能。（训练吞吐量和能源消耗）</p>
<p>Melon是基于我们对利用两种潜在技术的洞察而明智地构建的，这两种技术在云上还没有得到很好的探索</p>
<ol>
<li>micro batch</li>
<li>recomputation</li>
</ol>
<p>微批处理最初是为分布式学习中的跨gpu并行而提出的，但很少用于减少数据中心的内存使用，因为它会影响硬件并行性。由于移动设备的硬件容量有限，这个缺点可以很好地缓解。另一个缺点是，如果模型包含批量归一化(BatchNormalization, BN)层，在批处理中引入跨样本依赖性，则微批处理不能保证数学等效性。因此，对于具有 BN 层的模型，Melon 利用重新计算。重新计算可以通过丢弃和重新计算中间张量来帮助节省内存，但与内存交换相比，由于计算开销，通常认为云上的效率较低。正如本文后面所演示的，通过我们的新设计和技术，重新计算的成本实际上是可以接受的设备上训练。</p>
<h5 id="Challenges-and-techniques"><a href="#Challenges-and-techniques" class="headerlink" title="Challenges and techniques"></a>Challenges and techniques</h5><p>对于给定的DNN, Melon自动生成执行计划，指导不同内存预算下的训练行为。每个执行计划都详细阐述了内存张量分配、微批处理大小和重新计算调度策略，以在指定的内存预算下实现最佳性能。值得注意的是，Melon并不是简单地建立在微批处理和重新计算的组合之上。相反，我们遇到了以下独特的挑战，并通过我们的新技术解决了它们。</p>
<ul>
<li>Lifetime-aware memory pool</li>
</ul>
<p>首先，我们在模型训练期间观察到大量的内存碎片。设备上训练框架通常维护一个大型内存池来管理权重和中间激活。然而，由于分配策略不同和各种内存访问模式 [6, 26 , 45 ]，池的内存空间连续变为小块。根据我们的测量，在DNN训练期间，这些现有内存池的浪费内存可以达到42%。为了处理内存碎片，Melon 使用特定于模型的用户空间内存池，该池结合了静态内存访问模式的知识（即，当需要张量以及它在模型训练期间需要多少内存或称为生命周期）它基于一项简单但关键的观察，即训练期间生成的数万个张量具有多样化的生命周期。一个张量在内存中保留的时间越长，它就越可能与其他张量产生“干扰”。因此，Moon 使用贪心算法在低内存地址放置长期张量，以更好地整合内存池。【俄罗斯方块】</p>
<ul>
<li>Memory-calibrated progressive recomputation</li>
</ul>
<p>内存校准渐进重新计算。为了将所提出的生命周期感知内存池与重新计算技术相结合，Melon面临着“鸡还是蛋”困境。内存池将所有张量的生命周期作为输入，并生成张量分配计划以及所需的内存池的总大小。但是，重新计算将池大小作为输入来做出决定，这可能会影响池的策略。单独优化它们中的每一个并简单地应用一个顶部会导致次优性能.因此，Moon 提出了一种通过校准内存池进行重新计算算法。按照执行顺序，当使用的内存大于预算时，Melon丢弃一个分配的张量，并校准那些生命周期与被丢弃的张量有“干扰”的张量的位置。当内存中不存在当前运算符所需的张量时，Moon 搜索要重新计算的所有源张量并为它们分配内存。分配是通过扩展“时间轴”来执行的，根据它们的生命周期将张量添加到池中。然后Melon以与前面提到的相同的方式校准池。</p>
<ul>
<li>On-the-fly memory budget adapting</li>
</ul>
<p>前两种技术仅适用于静态内存预算。然而，移动设备是多应用程序或多任务环境。因此，Melon应该支持动态内存预算。简单地中止当前的批训练会导致大量的计算资源浪费，例如几十秒。为了以较低的开销快速响应新的内存预算，Melon使用了一种动态内存适应机制。一旦有了新的预算，Melonfirst就加载新的执行计划，并扩大&#x2F;缩小内存池以满足内存预算。然后，它根据新计划重新计算应保存在内存中但尚未呈现的张量（由于新&#x2F;旧计划的差异或丢弃的内存空间）。然后Melon调整张量位置以适应新计划并继续训练。通过这种方式，Melon通过重用先前计算结果的一部分来减少切换开销，而不是从一开始就重新执行DNN。</p>
<h5 id="Implementation-and-evaluation"><a href="#Implementation-and-evaluation" class="headerlink" title="Implementation and evaluation"></a>Implementation and evaluation</h5><p>我们已经在我们将在第 5 节中展示的最先进的设备上训练库 MNN [26 ] 上完全实现了 Meon 和 4 个基线。决策阶段一次性在客户端运行，例如，当应用程序安装时，会给开发人员带来几乎零的编程工作。然后，我们在四种典型的DNN模型和四种商用Android设备上进行了广泛的实验。实验结果表明，与vanilla MNN相比，Melon足以支持更大批量(4.33×)的设备上训练，这比所有基线都要显著得多。如此大的批大小使得melon在一个端到端的学习任务中，训练作业的收敛速度加快了3.48×，收敛精度提高了2.2%。为了支持相同的大批量，与基准相比，melon降低了49.43%的能源消耗。此外，与重启机制相比，Melon节省了高达95.73%的内存预算切换开销。</p>
<h5 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h5><ul>
<li>我们彻底测量和探索有希望的内存优化对设备上训练的深刻影响。</li>
<li>我们设计并实现了第一个内存优化的设备上训练框架Melon，具有三种新技术，即生命周期感知内存池、内存校准渐进重新计算和动态内存适应</li>
<li>我们用具有代表性的DNN模型和商用移动设备评估Melon。结果证明了它的有效性。</li>
</ul>
<h3 id="2-MOTIVATION-AND-PRELIMINARIES"><a href="#2-MOTIVATION-AND-PRELIMINARIES" class="headerlink" title="2 MOTIVATION AND PRELIMINARIES"></a>2 MOTIVATION AND PRELIMINARIES</h3><p>在本节中，我们将简要介绍设备上的训练，并进行初步实验。</p>
<h4 id="2-1-On-Device-Training"><a href="#2-1-On-Device-Training" class="headerlink" title="2.1 On-Device Training"></a>2.1 On-Device Training</h4><p>设备上训练的能力是许多高级学习场景的基础，如联邦学习[41]和边缘设置下设备上的迁移学习。着公众对数据隐私的日益关注和GDPR[4]等相关法律颁布，这种需求不断增长。</p>
<p>设备上训练通常采用随机梯度下降 (SGD) [9]，其中训练时期可以分为一些小批量。每一批的训练都应该经历一个完整的数据流：前向传递来计算损失，后向传递以获得梯度，以及基于梯度的参数更新。与模型推理（即预测）阶段不同，中间张量一旦它们已经被下一层使用，训练阶段需要保留前向传递期间生成的输出，直到它们在后向传递期间使用。因此，训练比推理需要更多的内存。</p>
<h5 id="Breakdown"><a href="#Breakdown" class="headerlink" title="Breakdown"></a>Breakdown</h5><p>我们使用最先进的设备上训练库 MNN 对 DNN 训练期间的峰值内存占用进行了细分分析。结果如图1所示，我们将内存使用分为3类，即&#x3D;&#x3D;权重内存(存储参数)，激活内存(存储中间输出)和优化器内存(存储梯度)。&#x3D;&#x3D;</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221211818878.png" srcset="/img/loading.gif" lazyload alt="图1:内存占用细分"></p>
<p>它表明，&#x3D;&#x3D;激活内存通常主导整体内存消耗，并与批处理大小成线性比例&#x3D;&#x3D;。这意味着我们要在设备上学习过程中优化这部分内存。</p>
<h4 id="2-2-The-Memory-Wall"><a href="#2-2-The-Memory-Wall" class="headerlink" title="2.2 The Memory Wall"></a>2.2 The Memory Wall</h4><p>在这里，一个直观但尚未探索的问题是:商用移动设备能否支持典型DNN模型的训练，以达到良好的精度?在实践中，机器学习界已经达成共识，&#x3D;&#x3D;大的批大小有助于稳定收敛方向&#x3D;&#x3D;。我们还对批量大小如何影响集中式（即单个 GPU 机器中的数据）和联邦设置（即假设数据以非 IID 方式分布在许多客户端上）中的模型收敛性进行了测量研究）。实验结果如表2所示。我们确认大尺寸需要确保良好的准确性和收敛速度。具体来说，对于联邦设置中批量大小为 128 的 MobileNetV2，训练过程在第 164 轮收敛，比批量大小为 32 的 MobileNetV2 快 45.73%。此外，测试精度高 3.94%。相同的观察结果可以在集中式设置中找到，&#x3D;&#x3D;其中使用更大的批量大小会导致 2% 的准确率或 39.02% 更快的收敛时间。&#x3D;&#x3D;</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221212605094.png" srcset="/img/loading.gif" lazyload alt="图2:不同批量训练的收敛速度"></p>
<p>然而，使用&#x3D;&#x3D;更大批量大小训练模型需要更多的内存容量也就不足为奇了&#x3D;&#x3D;。在实践中，商品移动设备不能充分支持大批量训练，即内存瓶颈。表 1 总结了内存墙如何影响设备上的训练。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221212642249.png" srcset="/img/loading.gif" lazyload alt="表1:内存墙在移动设备上的影响"></p>
<p>即使使用旗舰高端商品设备（Samsung 注意 10、8GB RAM），MNN 库只能支持 32 的批量大小，同时在集中式和联邦设置中导致更低的准确性和更多的训练轮次。</p>
<p>【？128轮的是怎么训练出来的】</p>
<h3 id="3-EXPLORING-EXISTING-TECHNIQUES"><a href="#3-EXPLORING-EXISTING-TECHNIQUES" class="headerlink" title="3 EXPLORING EXISTING TECHNIQUES"></a>3 EXPLORING EXISTING TECHNIQUES</h3><p>在本节中，我们首先检查最初为云设计的现有内存节省技术，并定量分析为什么这些技术不足以用于移动设备。然后我们探索新的设计空间，可能有助于节省训练内存消耗。</p>
<h5 id="Model-amp-gradients-compression"><a href="#Model-amp-gradients-compression" class="headerlink" title="Model &amp; gradients compression"></a>Model &amp; gradients compression</h5><p>量化[48,64]（打马赛克）被广泛采用，&#x3D;&#x3D;通过减少表示每个权重所需的比特数来压缩dnn。&#x3D;&#x3D;例如，8位和16位量化是压缩dnn最常见的解决方案，精度损失可以忽略不计[18,58]。在极端情况下，使用1位表示法已被证明是有效的[12,13,51]。然而，为了减少内存占用，低精度表示下的训练模型比推理更具挑战性，而且往往会导致模型精度的不可接受的下降。</p>
<p>计算和存储网络激活所需的内存远远超过存储模型本身所需的内存。两种可能的解决方案是将输入数据压缩到更小的尺寸，或者使模型“更窄”(例如，减少每个卷积的输出通道数量)。超过一定程度，这些技术中的每一种都必然导致准确性的损失——前者是因为它丢弃了数据中的信息，后者是因为它降低了模型的表达能力</p>
<p>【压缩带来精度下降】</p>
<p>我们意识到，基于fp32的训练和基于int8的训练之间的精度差距是无法通过先进的学习算法来消除的。最近的一项研究[78]提出了一种反向量化的损失感知补偿，但在CIFAR-10数据集上的精度下降高达7.9%。另一种最先进的基于整数的训练算法NITI[63]使用了离散参数更新方案，也显著降低了DNN的精度。更糟糕的是，在像FL这样的新兴学习模式中，这种准确性差距可能会被放大。我们在图3所示的初步实验中观察到了这种现象，其中我们比较了集中设置和联邦设置下NITI的收敛过程。它表明，与基于fp32的训练相比，NITI的准确性下降在联邦设置中更为明显。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221213714568.png" srcset="/img/loading.gif" lazyload alt="图3:联邦学习的准确性损失被放大了"></p>
<h5 id="Host-device-memory-swapping"><a href="#Host-device-memory-swapping" class="headerlink" title="Host-device memory swapping"></a>Host-device memory swapping</h5><p>主机和设备间的内存交换。云gpu通常配备专用的内存卡，这些内存卡之间的数据移动<strong>非常快</strong>，例如PCIe 5.0的128GB&#x2F;s。鉴于主存通常比GPU内存更丰富，之前的工作[42,50,52,61]探索了在DNN训练中使用主存作为外部数据备份。智能交换机制可以减少GPU上的内存占用，同时具有边际的吞吐量损失，因为CPU&#x2F;GPU内存之间的I&#x2F;O可以与训练重叠，并且可以完全覆盖开销。</p>
<p>然而，与云相比，交换不适用于移动设备，移动设备通常使用所有处理器的集成内存芯片。因此，交换只能在设备上的主内存和磁盘之间执行，其中带宽非常有限，例如，在表 3 中列出的设备上测试的 100-300MB&#x2F;s 用于写入操作。我们还将通过实验证明，基于交换的机制在 §6 中的设备上表现出较差的性能。</p>
<p>【交换带宽小，开销大】</p>
<h5 id="Activation-recomputation"><a href="#Activation-recomputation" class="headerlink" title="Activation recomputation"></a>Activation recomputation</h5><p>如前面提到的，前向传递期间生成的激活支配内存使用。因此，一些文献探索了在前向传递过程中丢弃中间激活，并在后向阶段需要重新计算它们。如前面提到的，前向传递期间生成的激活支配内存使用。因此，一些文献探索了在前向传递过程中丢弃中间激活，并在后向阶段需要重新计算它们。【数据库？】重新计算文献的一个关键主题是选择检查点，在此基础上提出了许多算法。</p>
<p>我们认为重新计算可能对设备上的学习有用，因为它不会衰减模型精度，并且不依赖于设备硬件之间的弱特征。然而，当前的算法基于一个简单的假设，即保留张量大小的总和等于总内存占用，当使用用户空间内存池时，这将不准确。据我们所知，它们都没有考虑内存池的影响。</p>
<p>【没有考虑内存池，时间换空间】</p>
<h5 id="Splitting-mini-batch-to-micro-batch"><a href="#Splitting-mini-batch-to-micro-batch" class="headerlink" title="Splitting mini-batch to micro-batch."></a>Splitting mini-batch to micro-batch.</h5><p>使用小批量 SGD 算法，权重梯度在批次中的所有样本中取平均值。因此，小批量可以进一步拆分为各种较小的批次，即微批次 [24, 55]，其梯度是所有微批次梯度的平均值。&#x3D;&#x3D;我们在图 1 中的测量表明激活内存大小与批量大小成正比&#x3D;&#x3D;，因此将小批量拆分为微批次可以显着减少所需的内存</p>
<p>&#x3D;&#x3D;微批次最初是为管道并行性而设计的，以实现高效的分布式机器学习&#x3D;&#x3D;。该技术很少用于云端以减少内存占用，主要是因为&#x3D;&#x3D;较小的微批大小不能充分利用云 GPU 的高并行性&#x3D;&#x3D;，如图 4 所示。小批量的时候cpu的并行性已经用满了。然而，在移动设备上，相对较小的批处理大小足以达到最大的硬件资源利用率。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221215815035.png" srcset="/img/loading.gif" lazyload alt="图4:使用小批量可以充分利用移动端硬件的并行性"></p>
<p>此外，对于具有BatchNormalization (BN)层2的DNN模型，无法保证微批的计算正确性，这涉及到样本间的数据依赖性。尽管提出了像GhostBN[21]这样的算法来解决这个问题，但统计变化仍然是不可避免的。因此，我们将微批次视为仅用于没有 BN 层的模型的设备上内存保存技术的机会。</p>
<p>在每个小批结束时，累积所有M个微批的梯度，并应用于更新所有加速器的模型参数。</p>
<p>例如，BatchNorm在训练期间对微批使用统计信息，但累积小批统计信息用于评估</p>
<p>微批处理(减少激活内存)。在基于小批量的训练算法中，可以通过网络一次性发送整个小批量，然后相应地更新权重，或者通过网络依次发送小批量的更小子集(称为微批量)，积累梯度，直到整个小批量被处理完毕</p>
<p>这在数学上等同于在DC-Transformer等模型中的标准训练，这些模型不包含批量归一化，并且确实不会导致DC-Transformer的准确性下降。然而，微批处理改变了批归一化层的统计特性;然而，我们发现，在WideResNet上，使用小至10的微批处理进行训练不会导致准确性的任何损失</p>
<h5 id="Summarized-Implications"><a href="#Summarized-Implications" class="headerlink" title="Summarized Implications"></a>Summarized Implications</h5><p>通过对现有技术的先前测量，我们发现移动场景与云场景之间存在差距。一方面，在云中广泛研究的交换和压缩不能很好地适应移动设备。另一方面，微批次带来了一个新的机会，由于移动设备的硬件容量有限，其缺点得到了缓解，重新计算技术足够通用，可以支持各种硬件和模型。这些发现表明，设备上的内存优化与云有很大的不同，导致我们将Melon构建为特定于移动的框架。特别是，Melon 需要改进适当的技术（微批次和重新计算），并首次将它们整合以获得节省内存的最好处。</p>
<h3 id="4-THE-DESIGN"><a href="#4-THE-DESIGN" class="headerlink" title="4 THE DESIGN"></a>4 THE DESIGN</h3><p>在本节中，我们将首先概述Melon，然后详细说明其每种新技术</p>
<h4 id="4-1-Overview"><a href="#4-1-Overview" class="headerlink" title="4.1 Overview"></a>4.1 Overview</h4><h5 id="Design-goal"><a href="#Design-goal" class="headerlink" title="Design goal"></a>Design goal</h5><p>Melon的目标是在&#x3D;&#x3D;给定的批大小和内存预算下最大化模型的训练性能&#x3D;&#x3D;。在训练任务中，批处理大小通常由算法开发人员固定，而内存预算可以由应用程序或操作系统在运行时动态调整。</p>
<p>Melon改进了微批处理和重计算技术以节省内存，并与新的内存池结合以减少内存碎片。在训练没有引入交叉样本依赖的BN层的模型时，Melon采用了微批技术。Melon改进了微批处理和重计算技术以节省内存，并与新的内存池结合以减少内存碎片。在训练没有引入&#x3D;&#x3D;交叉样本依赖的BN层&#x3D;&#x3D;的模型时，Melon采用了微批技术。首先，从每个微批中&#x3D;&#x3D;聚合缓冲梯度需要时间，但与训练时间相比，开销微不足道(≤1%)。&#x3D;&#x3D;第二个开销是小批处理大小降低了内部操作执行的并行性。由于移动设备的硬件容量有限，这一开销也可以忽略不计，如§3所述。因此，我们认为Melon采用微批技术很好地解决了某些dnn的内存壁问题。</p>
<p>然而，BN 层成为 DNN 训练的事实标准（例如 ResNet [19] 和 Transformers [59]）。因此，Melon 更进一步，专注于支持通过重新计算包含 BN 层的通用 DNN 模型。Melonis 的关键设计，通过确定应该丢弃哪些张量或重新计算来最小化重新计算开销。然而，直接应用池和重新计算将面临一个两难境地，这两个困境都需要彼此的全局知识。为了解决这个问题，我们提出了一种新的重新计算机制，如§4.3所示</p>
<h5 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h5><p>如图5所示，Melon分两个阶段工作</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221224148015.png" srcset="/img/loading.gif" lazyload alt="图5:Melon概览"></p>
<ol>
<li>在决策阶段，Melon生成在不同内存预算下实现最佳性能的执行计划</li>
<li>在执行阶段，Moon 根据计划执行 DNN 训练。</li>
</ol>
<p>这样的两阶段设计是基于DNN训练过程中规则张量访问模式的机会，已经在现有的工作[46]中采用。注意，这两个阶段都在设备上运行，&#x3D;&#x3D;决策阶段在执行阶段之前自动触发。&#x3D;&#x3D;因此，这样的设计不会为开发人员引入任何额外的编程工作(例如，在我们的实现中只有一行shell命令)。</p>
<ul>
<li><strong>Decision stage</strong></li>
</ul>
<p>在训练DNN模型之前，Melon首先运行分析迭代，通过execution Profiler获得运行时信息：被分析的信息包含了在训练过程中生成的神经网络算子和张量，包括&#x3D;&#x3D;数据流依赖关系、每个张量的大小、每个算子的计算时间、每个张量的生命周期&#x3D;&#x3D;等。&#x3D;&#x3D;然后，概要信息被提供给执行计划生成器，它会生成执行计划来详细说明内存节省的细节，例如:（1）每个张量被放在哪里（2）哪些运算符需要重新计算。&#x3D;&#x3D;此外，执行计划还包含批量拆分策略，该策略指定了&#x3D;&#x3D;没有BN层的模型的拆分批次大小。&#x3D;&#x3D;由于该技术对训练过程的统计特征没有影响，我们只需使用设备支持的最大微批处理大小，以最小化通过聚合缓冲梯度引入的额外开销。下面的小节描述了Melon如何搜索最佳执行计划。每个执行计划对应一个内存预算，因此Melon预先定义了一组内存预算，并为每个预算生成最佳的执行计划，这些计划将与用于执行阶段的模型一起存储在本地。适应一组预定义的预算，而不是任意预算，简化了Melon的内存优化设计。成本是微不足道的，因为每个执行计划在我们的实现中只需要几个 KB。</p>
<ul>
<li><strong>Execution stage</strong></li>
</ul>
<p>一旦训练任务开始，Melon 的训练引擎根据当前内存预算加载适当的执行计划，并执行计划指导的训练。&#x3D;&#x3D;当内存预算发生变化时，Melon 会检查是否需要加载新计划。如果需要，Melon 根据第 4.4 节中讨论的技术快速切换到新计划。&#x3D;&#x3D;</p>
<p>为了尽量减少开发人员的手动工作，Moon 的决策阶段在设备上运行以自动生成执行计划。这些计划可以存储在本地存储上，因此它们只需要生成一次，例如，当安装应用程序或从服务器获取新模型时。</p>
<h4 id="4-2-Lifetime-Aware-Memory-Pool"><a href="#4-2-Lifetime-Aware-Memory-Pool" class="headerlink" title="4.2 Lifetime-Aware Memory Pool"></a>4.2 Lifetime-Aware Memory Pool</h4><p>用户空间内存池[77]是训练框架[6,26,45]管理内存的常用方法。它避免了与操作系统频繁交互以分配&#x2F;释放内存块的高开销。【生活费】如今，这些框架使用的内存池按顺序为张量分配内存，并在每个分配后更新池信息。然而，这种设计忽略了 DNN 训练迭代重复的独特特征，并可能导致严重的内存碎片，例如，使用 MNN 在相同的设置中浪费高达 42% 的内存空间</p>
<h5 id="4-2-1-Opportunity-and-Heuristics"><a href="#4-2-1-Opportunity-and-Heuristics" class="headerlink" title="4.2.1 Opportunity and Heuristics."></a>4.2.1 Opportunity and Heuristics.</h5><p>改进内存布局的一个机会是在批处理粒度的训练中保持一致的内存操作。基于概要的内存操作信息，可以用最小的内存大小构建最佳布局。图6显示了如何通过更好的布局来节省内存的示例。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221230605606.png" srcset="/img/loading.gif" lazyload alt="图6:改善内存布局"></p>
<p>With the on-demand strategy shown in Figure 6(a), the 𝑇2 is assigned to an address aside 𝑇1. After 𝑇1 is released, 𝑇4 cannot fit into the memory space below 𝑇2, therefore it should be located in the address above 𝑇2. Consequently, the total memory footprint is the sum of 𝑇1, 𝑇2 and 𝑇4. In the optimized allocation strategy shown in Figure 6(b), the memory footprint size can be reduced to the sum of 𝑇2 and 𝑇4.</p>
<p>但是，解决上述内存节省问题类似于2DSP问题[8]——一个经典的NP-Hard问题。该问题的输入由数千个张量组成，因此不可能穷尽最优解。为了获得一个接近最优的解决方案，很少的努力已经投入。&#x3D;&#x3D;这些方法通常以“大张量优先”的贪婪方式执行内存分配。&#x3D;&#x3D;相反，我们发现张量的生命期(经度)会对布局效果产生巨大影响。直观地说，一个张量在内存池中停留的时间越长，它与其他张量的“干扰”就越多，因为它将内存池分割为两个由时间戳给出的分离段。事实上，这种生命周期多样化在 DNN 训练中很普遍，可以分为两大类。</p>
<ol>
<li>&#x3D;&#x3D;激活跨越很长的生命周期，即在正向传递时产生，在反向传递时释放。与堆栈数据结构类似，它遵循“先产生后发布”(First Produce Last Release, FPLR)顺序，即越早产生激活，越晚释放激活。&#x3D;&#x3D;</li>
<li>其他临时张量的生命周期比激活要短得多，只跨越几个甚至一个算子。这样的观察指导我们根据每个张量的生命周期以贪婪的方式分配它们，以近似这个类2dsp问题的最优解。</li>
</ol>
<h5 id="4-2-2-Our-Approach"><a href="#4-2-2-Our-Approach" class="headerlink" title="4.2.2 Our Approach"></a>4.2.2 Our Approach</h5><p>基于前面的观察，melon采用了一种张量生命周期感知算法来优化内存布局。&#x3D;&#x3D;关键思想是将长生命期张量放在短生命期张量下面，以巩固整个内存布局。&#x3D;&#x3D;Melon迭代地将具有最长生存期的张量放置在可能的最低内存地址上。当一个张量的尾部超过当前池的大小时，内存池就会扩展。这个过程以贪婪的方式执行。利用每个张量的概要信息，Melon将内存池和张量抽象为一个2D轴和矩形，如图6所示.内存地址可以表示为相对于池底部的偏移量。在执行阶段，Melon一次请求所有的内存空间。在为每个张量分配内存时，Melon只是根据执行计划为每个张量分配池中特定的地址。</p>
<h4 id="4-3-Memory-Calibrated-Progressive-Recomputation"><a href="#4-3-Memory-Calibrated-Progressive-Recomputation" class="headerlink" title="4.3 Memory-Calibrated Progressive Recomputation"></a>4.3 Memory-Calibrated Progressive Recomputation</h4><h5 id="4-3-1-Problems-of-Existing-Techniques"><a href="#4-3-1-Problems-of-Existing-Techniques" class="headerlink" title="4.3.1 Problems of Existing Techniques"></a>4.3.1 Problems of Existing Techniques</h5><p>首先，先前的重新计算策略[11,46]只考虑前向传播中产生的激活。然而，我们观察到在前向和后向阶段都产生了许多碎片化和临时张量，例如图 7中的F3和B3。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221232121580.png" srcset="/img/loading.gif" lazyload alt="图7:Melon的重新计算工作流程"></p>
<p>&#x3D;&#x3D;其次，先前的工作使重计算策略仅基于朴素峰值内存，即所有激活张量的和。&#x3D;&#x3D;事实上，在MobileNetV2与MNN的训练向前传递过程中产生的1800多个张量中，只有大约200个是需要长期使用的激活。虽然其他张量只存在很短的生命周期，但它们会占用重要的内存空间，并导致内存使用溢出。据我们所知，它们都没有考虑内存池的影响，重新计算策略会导致不准确的结果。</p>
<h5 id="4-3-2-Our-Approach"><a href="#4-3-2-Our-Approach" class="headerlink" title="4.3.2 Our Approach"></a>4.3.2 Our Approach</h5><p>为此，Melon引入了一种不同的重计算机制，该机制全面考虑了内存池的影响。但是，如§4.2所述，&#x3D;&#x3D;池需要所有张量的全局知识来进行分配决策，&#x3D;&#x3D;即所有张量的生命周期都可能受到重新计算的影响。只有当池的信息可访问时，即当前张量的尾部是否超过内存预算时，才能制定重计算策略。换句话说，内存池和重新计算都需要彼此的完整知识才能做出良好的决策。&#x3D;&#x3D;为了解决这个困境，我们引入了内存校准的渐进式重新计算&#x3D;&#x3D;，如算法 1 所示。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221221233043499.png" srcset="/img/loading.gif" lazyload alt="image-20221221233043499"></p>
<p>Melon将整个算子图作为输入，并对每个张量进行平等的重新计算。</p>
<p>在确定要丢弃哪些张量进行重新计算时，&#x3D;&#x3D;Melon引入了度量三角形每秒(TPS) (Eq 1)来估计重新计算每个张量的好处，即大小更大、释放寿命更长&#x3D;&#x3D;、重新计算时间更短的张量具有更高的优先级，可以稍后丢弃并重新计算。释放生命期定义为丢弃和重新计算之间的生命期。更大的大小和更长的释放生命周期表明丢弃张量可以在内存池中带来更多可用空间，如图6所示</p>
<p>Melon的重计算机制是以递进的方式进行的。它首先通过原始的执行流(第1行)初始化内存池，然后按照原始的执行流(第12行)逐个模拟执行操作符。在模拟执行期间，每个张量都被分配了它在池中的确切位置的地址。</p>
<p>&#x3D;&#x3D;当一个张量的尾部超过内存预算时，&#x3D;&#x3D;重新计算机制被触发(第6-10行)。重计算机制不断丢弃TPS值最大的张量，并对内存池进行校准，直到池大小不大于预算。下一个算子的输入张量被认为没有被丢弃(第7行)。在丢弃过程中，池在当前步骤释放张量(如图6所示，删除部分矩形)。一旦一个张量被丢弃，Melon校准之后产生的所有生命周期与它有“干扰”的张量的内存地址(第8行)</p>
<p>当一个张量的尾部超过内存预算时，重新计算机制被触发(第6-10行)。重计算机制不断丢弃TPS值最大的张量，并对内存池进行校准，直到池大小不大于预算。下一个算子的输入张量被认为没有被丢弃(第7行)。在丢弃过程中，池在当前步骤释放张量(如图6所示，删除部分矩形)。一旦一个张量被丢弃，Melon校准之后产生的所有生命周期与它有“干扰”的张量的内存地址(第8行)。这里的干涉被定义为两个张量寿命的重叠。如图6所示，此时右侧的张量将“下沉”到较低的地址。这样的丢弃过程在内存张量上重复，直到池的大小没有超过内存预算。</p>
<p>当当前运算符所需要的张量在内存中没有出现时，算法分配内存并与它的源张量一起重新计算(第13-19行)。源张量被递归收集，直到输入张量出现在内存中(第14行)。通过这样的机制(第7行和第14行)，保证了操作符之间的输入依赖。重新计算张量会导致大量的时间开销，因为它会产生一些应该添加到池中的张量。因此，<strong>Melon需要扩展内存中已经存在的张量的生命周期</strong>(图6中的时间轴)。首先，池将生命周期从当前步骤延长到这些张量的生命周期长度，并且当前时间的所有“矩形”将向右移动，表明它们将在稍后生成。然后将这些张量添加到池中，池以同样的方式校准寿命与它们有干扰的张量。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222161555781.png" srcset="/img/loading.gif" lazyload alt="图7:melon的工作流程"></p>
<p>图7演示了Melon的重新计算是如何工作的。假设拓扑顺序上的操作图为F1→F5, B5→B1 .F表示前向计算，B表示返向计算。图中的黑色箭头表示操作符图中的数据流。活化是由F1, F2和F4,F3和F5是产生临时张量的算子。假设内存预算在𝐹3之后用完，即使𝐹3的输出不是激活，具有最大TPS的𝐹2的输出也会被丢弃。在这种情况下，就地内存分配无法工作，因为应该保留F1和표F2的输出，直到它们在向后传递中没有使用。在反向传播期间，𝐵3作为中间算子来支持 𝐵2 的计算。即使在反向传递中，内存占用大小也可能超过预算，然后算法选择一个张量以与前向传递相同的方式被赶出。当张量被驱逐时，将更新每个张量的TPS。</p>
<h4 id="4-4-Memory-Budget-Adaptation"><a href="#4-4-Memory-Budget-Adaptation" class="headerlink" title="4.4 Memory Budget Adaptation"></a>4.4 Memory Budget Adaptation</h4><p>移动设备通常支持多应用程序执行环境，其中每个应用程序&#x2F;服务分配的硬件资源可以高度动态。这种内存适应信号可能来自操作系统或应用程序本身。为了适应新的内存预算，melon需要(i)快速响应变化，例如，在需要时释放内存，以及(ii)最小化切换执行计划的开销。</p>
<p>对于扩大内存预算的情况，Melon简单地采用了惰性切换策略，即等待当前批处理的训练结束，然后切换到新的执行计划。然而，在内存预算缩水的情况下，这种懒惰的策略是不可行的，因为内存需要立即释放给应用程序或操作系统。另一种直观的方法是停止-重新启动，这意味着整个内存池将被重新分配，当前批处理的所有中间结果将被丢弃。虽然它可以立即释放内存，但重新执行操作符也会导致非常高的开销。</p>
<p>为此，我们提出了一种动态内存调整机制，可以快速响应内存预算的变化，并根据保留的(部分)结果恢复执行。一旦有了新的预算，Melon首先缩小池大小以满足内存预算。Melon从当前内存池开始保存新内存预算的大小，并通过realloc函数转储剩余的内存预算。然后它加载新的执行计划并跳转到当前操作符的执行点。</p>
<p>下一个关键步骤是恢复新计划的内存布局。我们使用 A、B 和 C 分别表示旧执行计划的内存张量集、新执行计划中应该在内存中呈现的张量集和丢弃的张量集。Melon 仅在内存中的 (𝐴 −𝐶) ∩𝐵 中保留张量并转储其他张量。请注意，转储操作在物理上不需要任何内存操作，但只标记相应的内存块free。然后，Melon 将旧执行计划的剩余张量的内存地址调整为新张量。最后，Melon 根据模型的执行顺序重新计算 𝐶 − (𝐴 − 𝐵) 中的张量。通过前面的步骤，Melon可以成功地恢复内存布局，并使用新的执行计划恢复训练，而不是从头开始重新执行之前的操作符。</p>
<h3 id="5-IMPLEMENTATION"><a href="#5-IMPLEMENTATION" class="headerlink" title="5 IMPLEMENTATION"></a>5 IMPLEMENTATION</h3><p>我们已经在MNN (v1.1.0)[26]上完全实现了Melon的原型。据我们所知，MNN、TFLite[5]和DL4J[3]是唯一三个支持在Android设备上训练现代dnn的库。我们使用MNN是因为它在速度和内存使用方面优于其他两种，正如我们的测量(表2)和之前的工作[10]所示。注意，Melon的设计足够通用，也可以集成到其他库中。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222164639140.png" srcset="/img/loading.gif" lazyload alt="表2:使用MNN更优"></p>
<p>我们的原型主要包括两个模块(c++中总共6.4k LoC):1)离线分析和在线内存优化执行的执行引擎;(2)执行计划生成器生成不同内存预算下的最优执行计划。请注意，它们都以自动的方式在设备上运行，不需要开发人员付出额外的努力。</p>
<p>虽然MNN在概念上与Android和iOS设备兼容，但我们的原型目前针对的是Android设备，因为有许多特定于操作系统的内存操作。目前原型主要支持移动CPU上的训练，因为mnn对gpu上训练相关算子的支持非常有限，即使支持的模型与CPU[10]相比性能也很差。值得一提的是，Melon的设计主要与移动GPU兼容，但需要解决一些独特的挑战，例如内存适配过程中的内存复制开销。据我们所知，MNN 是目前唯一支持移动 GPU 的设备上训练库，作为这项工作的发布。§6.7 中的评估将证明Melon 的兼容性和通用性。</p>
<h5 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h5><p>我们还从以往的文献中吸取教训，实施了四个基线。请注意，部分前期工作的源代码无法获取，我们尽量根据相应的论文进行复现。为了公平比较，我们在MNN上重新实现了它们。</p>
<ul>
<li>理想：我们假设设备配备了无限内存容量的理想情况，通过直接重用设备内存来实现（从而影响计算正确性）。这个baseline提供了Melon或其他基线可以达到的严格的性能上限。</li>
<li>vDNN[52]: 基于交换的运行时内存管理解决方案，实现dnn内存使用虚拟化。这里我们在内存和磁盘(设备上的内部存储)之间交换数据。</li>
<li>Sublinear: 一种分层重计算算法，当当前内存使用超过由启发式确定的阈值时，清除一个张量</li>
<li>Capuchin[46]：一种有效的基于张量的优化算法，结合了交换和重新计算。</li>
</ul>
<h3 id="6-EVALUATION"><a href="#6-EVALUATION" class="headerlink" title="6 EVALUATION"></a>6 EVALUATION</h3><p>在本节中，我们从各个方面评估Melon和基线，以演示Melon的效率。</p>
<h4 id="6-1-Experiment-Settings"><a href="#6-1-Experiment-Settings" class="headerlink" title="6.1 Experiment Settings"></a>6.1 Experiment Settings</h4><h5 id="Models-and-datasets"><a href="#Models-and-datasets" class="headerlink" title="Models and datasets"></a>Models and datasets</h5><p>我们使用表3中列出的4种典型的CNN模型对Melon进行评估，这些模型广泛应用于移动设备，包括MobileNetV1 [22]， MobileNetV2 [53]， SqueezeNet[25]和ResNet-50[19]。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222165635792.png" srcset="/img/loading.gif" lazyload alt="image-20221222165635792"></p>
<p>对于每个模型，我们实现了两个版本:带BN层和不带BN层(在每个卷积层之后添加)。我们没有包括语言模型，因为MNN缺乏这样的支持。我们使用CIFAR-100数据集，输入大小调整为224×224×3[25,53]。</p>
<h5 id="Hardware-setup"><a href="#Hardware-setup" class="headerlink" title="Hardware setup"></a>Hardware setup</h5><p>我们在4个不同soc和内存容量的Android设备上进行实验(表3)。我们一直在大核上运行Melon等基线，以达到公平的比较。</p>
<h5 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h5><p>我们在训练期间测量内存使用、能量消耗和吞吐量。procrank 监控的内存使用。能量消耗是通过 Android 的 vFS (&#x2F;sys&#x2F;class&#x2F;power_supply) 计算的。训练吞吐量定义为每秒训练的数据样本的数量。</p>
<h4 id="6-2-Overall-Performance"><a href="#6-2-Overall-Performance" class="headerlink" title="6.2 Overall Performance"></a>6.2 Overall Performance</h4><p>我们首先从3个主要方面衡量Melon的整体性能，即达到相同吞吐量时支持的最大批大小，训练收敛性能，以及更大批的训练吞吐量。</p>
<h5 id="Maximal-batch-size-supported"><a href="#Maximal-batch-size-supported" class="headerlink" title="Maximal batch size supported"></a>Maximal batch size supported</h5><p>我们首先测量不同吞吐量所能达到的最大批大小。我们在三星Note 10上使用MobileNetV2和SqueezeNet(都带有BN层)进行实验。结果如图8所示。结果表明，Melon的内存优化在不同的吞吐量下都能很好地扩展，并且总是显著优于其他方法。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222170146596.png" srcset="/img/loading.gif" lazyload alt="image-20221222170146596"></p>
<p>例如，当吞吐量为2.39fps时，Melon可以训练批量大小为208的MobileNetV2，而其他基线的批量大小都小于96。</p>
<h5 id="End-to-end-convergence-performance"><a href="#End-to-end-convergence-performance" class="headerlink" title="End-to-end convergence performance"></a>End-to-end convergence performance</h5><p>我们还评估了melon在集中式和联邦设置下端到端学习任务中的表现。我们使用的数据集是CIFAR-100。对于联邦设置，我们用10个设备初始化训练过程，所有设备上的数据分布是非iid[29]。每个设备上的数据只覆盖类的一个子集。由于实验是为了说明Melon在交易批量大小和训练速度上的有效性，因此我们在联邦学习中不考虑设备异质性[72]，只考虑三星Note10上的训练速度。对于联邦和集中式场景，其他设置都是相同的。</p>
<p>如图9所示，通过支持更大的批大小，Melon获得了比原始MNN更高的收敛精度，在联邦设置下，MobileNet-V2和SqueezeNet分别为3.94%和3.20%。在集中设置下，Melon的收敛精度分别提高了1.98%和2.04%。另一方面，Melon显著减少了训练时间，达到相同的精度。例如，与原始MNN相比，MobileNetV2和Squeeze-Net的收敛精度分别减少2.80×和3.48×的时间(58.22%和59.18%)。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222170958494.png" srcset="/img/loading.gif" lazyload alt="图9:melon支持更大批量训练"></p>
<h5 id="Throughput-with-the-same-batch-size"><a href="#Throughput-with-the-same-batch-size" class="headerlink" title="Throughput with the same batch size"></a>Throughput with the same batch size</h5><p>然后，我们通过改变不同的(放大的)批大小来全面研究Melon的训练性能，这些批大小在不使用内存节省技术的情况下无法训练。</p>
<p>实验在4个设备上进行，2个是有BN层的模型，2个是没有BN层的模型。对于每个组合，我们选择2-3个批大小，例如，如果原始最大批大小是32，我们使用64、96和128作为测试批大小。结果分别显示在图11和图12中</p>
<p>我们的主要观察结果是，Melon的性能始终显著优于其他备选优化基线，与Ideal基线相比，它的性能通常与Ideal基线相似。例如，在BN层模型上(图11)，Melon的吞吐量比vDNN高1.51× - 3.49×higher，比sublinear高1.13× - 3.86，比Capuchin高1.01× - 4.01。Melon实验表明，其优势在较大的batch size上更加明显，例如在64和128 batch size的红米Note9 Pro上训练MobileNetV2时，分别比Capuchin提高了3.25×和3.34×。然而，Melon和Ideal基线之间的性能差距总是随着更大的批处理规模而增加(例如，在红米Note9 Pro上训练MobileNetV2时，从10.67%增加到21.21%)，因为Melon需要更积极地丢弃和重新计算引起计算开销的张量。在基线中，vDNN在大多数情况下表现最差，因为在§3中提到的移动设备上的数据交换速度有限。注意Capuchin的改进几乎得益于重新计算，因为交换引入了严重的同步开销。它还忽略了内存池的影响，这意味着它将浪费更多的空间，重新计算更多的张量来支持更大的批处理大小，导致吞吐量损失。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222171615052.png" srcset="/img/loading.gif" lazyload alt="image-20221222171615052"></p>
<p>对于没有BN层的模型(图12)，Melon在任意批量大小下几乎可以赶上Ideal基线的性能(损失仅小于1%)。与其他优化基准相比，性能提升也非常显著，魅族16t平均提升1.77×(最高2.66×)，红米Note8平均提升1.57×(最高2.15×)。这是因为，对于没有BN层的模型，Melon利用了微批处理技术，如§3所述，性能下降很小。注意，当批大小相对较小时，其他基线也可以实现相对较高的性能。这是因为去掉了所有的BN层，BN层的数量接近卷积层。将有更少的激活和更少的计算，导致高性能的基线。然而，与Melon不同的是，这些基线的性能随着大批处理而衰减。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222172934586.png" srcset="/img/loading.gif" lazyload alt="image-20221222172934586"></p>
<h4 id="6-3-Memory-Budget-Adaptation"><a href="#6-3-Memory-Budget-Adaptation" class="headerlink" title="6.3 Memory Budget Adaptation"></a>6.3 Memory Budget Adaptation</h4><p>然后，我们评估了§4.4中所介绍的melon的内存预算适应设计。我们关注内存预算减少的情况，因为它在实践中更具挑战性。实验使用三星Note 10上的2个型号(MobileNetV2和SqueezeNet)。我们选择了2个不同的适应场景:批大小为128时从6GB切换到5GB，批大小为64时从4GB切换到3GB。注意，在每种情况下，如果不进行内存优化，内存预算都不足以训练批处理大小。我们还选择了3个自适应点，即当执行进度达到25%，50% 和 75%。本实验比较的基线是停止重启，如第 4.4 节所述。</p>
<p>结果如图 10 所示。适应开销是新计划在适应发生时达到与旧计划相同的运算符的时间成本，归一化为在适应时简单地丢弃所有张量的停止重启方法。相比之下，Moon 产生的适应开销要少得多，即 4.27%-54.50%。Meon 的开销在后验执行点增加，主要是因为要重新计算的张量数以恢复新执行计划的内存布局会增加。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222173253220.png" srcset="/img/loading.gif" lazyload alt="image-20221222173253220"></p>
<p>为了进一步了解自适应性能，我们还将开销分解为两大类：内存中的张量重新排列和根据新的执行计划重新计算丢失的张量。图10显示，在大多数情况下，重新计算开销占总调整开销的主导地位，尤其是在后执行点。这是因为在移动设备上，内存移动速度比计算速度快得多。</p>
<h4 id="6-4-Energy-Consumption"><a href="#6-4-Energy-Consumption" class="headerlink" title="6.4 Energy Consumption"></a>6.4 Energy Consumption</h4><p>由于移动设备的电池容量有限，能耗是另一个需要优化的关键指标。尽管Melon主要针对高训练吞吐量而不是降低能耗进行了优化，但我们仍然在这方面与其他基线一起对其进行了评估。在这里，我们在魅族16t设备上测试了两种型号（MobileNetV2和带有BN层的SqueezeNet）。结果如图13所示，数字标准化为Idealbaseline。</p>
<p>这表明 Mlon 显着降低了与基线相比的能量消耗，即 22.00%-49.43%。与交易基线相比，Moon 的能量消耗平均仅为 11.4%，而最佳情况下仅低 2.1%。Moon 的改进主要来自训练时间的减少。与训练吞吐量相比，vDNN 的性能得到了显着提高，因为读取&#x2F;写入操作的能量比计算少（约 2.5 倍的差距）。然而，由于训练时间变长，它仍然消耗比 Melon 更多的能量。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222173543993.png" srcset="/img/loading.gif" lazyload alt="image-20221222173543993"></p>
<h4 id="6-5-Ablation-Study"><a href="#6-5-Ablation-Study" class="headerlink" title="6.5 Ablation Study"></a>6.5 Ablation Study</h4><p>我们进一步对每种技术带来的好处进行了细分分析，即分别具有生命周期感知内存池或内存校准的渐进式重新计算。我们评估了每种方法可以通过不同吞吐量实现的最大批量大小。我们在三星 Note10 上使用 Copy2 和 SqueezeNet 进行实验。结果如图 14 所示。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222173619623.png" srcset="/img/loading.gif" lazyload alt="image-20221222173619623"></p>
<p>我们观察到这两种技术对改进都有不小的贡献。例如，当MobileNetV2的吞吐量为3.07fps时，我们的生命周期感知内存池和重新计算技术可以达到的最大批处理大小分别为40和80。结合它们，批大小可以增加到112，这几乎是线性成比例的。由于内存访问模式对于一个模型和一个批处理大小保持相同，因此池带来的改进在不同的吞吐量上保持相同。我们还发现，对于吞吐量较低的两个模型，Melon带来的改进大于pool和recomputation带来的改进之和。原因是随着批大小的增加，生命周期长的张量越来越少，这可以引入更多的机会来执行生命周期感知分配，如图6所示。</p>
<h4 id="6-6-Complexity-Analysis"><a href="#6-6-Complexity-Analysis" class="headerlink" title="6.6 Complexity Analysis"></a>6.6 Complexity Analysis</h4><p>我们衡量算法的成本，即生成执行计划的时间。首先，分析的时间等于训练一批的时间，与图9所示的整个训练过程相比，这几乎可以忽略不计。注意，在这个过程中我们重叠了所有张量，即所有张量共享同一块内存，因为统计值对分析没有影响。例如，使用三星Note10对批大小为32的MobileNetV2进行分析训练，耗时约10.3s，耗时约147MB。记录每个操作员延迟的额外时间也可以忽略不计。</p>
<p>Melon线下时间的主要来源是执行计划的生成。我们以在三星Note10上训练SqueezeNet为例，测量了这个开销来分析算法的复杂性。我们使用的批大小从64到172，步骤为16。实验结果表明，生成一个计划平均需要10.9s。由于生成的计划可以永久存储，所以这种延迟只会引起一次射击，因此在实践中，我们的算法的成本也是可以接受的</p>
<h4 id="6-7-GPU-support"><a href="#6-7-GPU-support" class="headerlink" title="6.7 GPU support"></a>6.7 GPU support</h4><p>我们进行了一个实验来探索Melon在移动GPU上的性能。我们测量支持的最大批大小。设置与§6.2相同。结果如图15所示。这表明Melon在不同的吞吐量下也可以达到最大的批量大小。但是，结果不如在CPU上那么令人印象深刻，因为Melon主要针对CPU进行了优化。</p>
<p><img src="https://nuaapeter.oss-cn-nanjing.aliyuncs.com/image-20221222174919246.png" srcset="/img/loading.gif" lazyload alt="image-20221222174919246"></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" class="category-chain-item">论文笔记</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/MobiSys22/">#MobiSys22</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Mobisys22 Melon笔记</div>
      <div>http://seupeter.cn/2022/12/27/Mobisys22-Melon笔记/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Peter Wan</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年12月27日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/12/31/%E6%9D%8E%E6%B2%90%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="动手学深度学习00：环境搭建">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">动手学深度学习00：环境搭建</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/12/20/picgo-%E5%9B%BE%E5%BA%8A%E6%90%AD%E5%BB%BA/" title="picgo 图床搭建">
                        <span class="hidden-mobile">picgo 图床搭建</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      

    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <span>Copyright</span> <i class="iconfont icon-copyright"></i> <span>2022-2023 Peter Wan 版权所有</span> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
